{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atonbom/KimPyRaptor/blob/main/Python/Tomato_Weight_Estimation_Github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWnNRwaNDmtN"
      },
      "source": [
        "#Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9zv8gDYXQIT"
      },
      "source": [
        "**Download datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXv5tGaLXFVC"
      },
      "source": [
        "#Merlice dataset\n",
        "#I build this dataset myself\n",
        "!gdown --id 1AQe2AyMREwxhtXEm24isNsKR7fjKkFs2\n",
        "!unzip '/content/tomato_dataframes_merlice.zip'\n",
        "!rm '/content/tomato_dataframes_merlice.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtXJNTYHXIKB"
      },
      "source": [
        "**Define dataframe for training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNwkAXjuDmNj"
      },
      "source": [
        "#tomato_dataframe = '/content/tomato_dataframe_tros.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_tros_5x.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_cocktail_tros.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_cocktail_tros_5x.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_cherry.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_cherry_5x.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_all_datasets_single.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_all_datasets_5x.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_all_datasets.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_merlice_batch01_20d.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe1x20d01_30.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_merlice_all.csv'\n",
        "tomato_dataframe = '/content/tomato_dataframe_merlice_all_but01.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_merlice_all_but02.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_merlice_all_but03.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_merlice_all_but04.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_merlice_all_but05.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_merlice_all_but06.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_merlice_all_but07.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_merlice_all_but08.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_merlice_all_30cm.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_merlice_all_60cm.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_merlice_all_3050cm.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_merlice_all_3060cm.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe_merlice_all_cleaned.csv'\n",
        "#tomato_dataframe = '/content/tomato_dataframe.csv'\n",
        "\n",
        "tomato_dataframe_test = '/content/tomato_dataframe_merlice_1x20d01.csv'\n",
        "#tomato_dataframe_test = '/content/tomato_dataframe_merlice_1x20d04.csv'\n",
        "#tomato_dataframe_test = '/content/tomato_dataframe_merlice_all_3060cm.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iByNncF3DQhp"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okN3rhpLXfg-"
      },
      "source": [
        "from pandas import read_csv\n",
        "dataframe = read_csv(tomato_dataframe, sep=',', header=0)\n",
        "print(dataframe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCMvPL1QRwij"
      },
      "source": [
        "# Regression Example With Boston Dataset: Baseline\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "# load dataset\n",
        "# use the correct seperator depending on your dataset\n",
        "dataframe = read_csv(tomato_dataframe, sep=',', header=0)\n",
        "#dataframe = read_csv(tomato_dataframe, sep=';', header=0)\n",
        "dataset = dataframe.values\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "#X = dataframe[['distance','depth', 'pixels']].values\n",
        "X = dataframe[['depth', 'pixels']].values\n",
        "Y = dataframe[['weight']].values\n",
        "\n",
        "# define base model\n",
        "def baseline_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    #model.add(Dense(3, input_dim=3, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dense(2, input_dim=2, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dense(1, kernel_initializer='normal'))\n",
        "    #Compile model\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    model.fit(X, Y, epochs=1000, verbose=0)\n",
        "    return model\n",
        "model = baseline_model()\n",
        "print('Done Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pZWoOJWW5nH"
      },
      "source": [
        "**Normalized model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY9xGxhzW9FF"
      },
      "source": [
        "# Regression Example With Boston Dataset: Standardized\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# load training dataset\n",
        "# use the correct seperator depending on your dataset\n",
        "dataframe = read_csv(tomato_dataframe, sep=',', header=0)\n",
        "# dataframe = read_csv(tomato_dataframe, sep=';', header=0)\n",
        "dataset = dataframe.values\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "#X = dataframe[['distance','depth', 'pixels']].values\n",
        "#X = dataframe[['distance','pixels']].values\n",
        "X = dataframe[['depth','pixels']].values\n",
        "Y = dataframe[['weight']].values\n",
        "\n",
        "# Parse numbers as floats\n",
        "X = X.astype('float32')\n",
        "Y = Y.astype('float32')\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "#model.add(Dense(3, input_dim=3, kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dense(2, input_dim=2, kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dense(1, kernel_initializer='normal'))\n",
        "model.add(Dense(3, kernel_initializer='normal'))\n",
        "model.add(Dense(1, kernel_initializer='normal'))\n",
        "\n",
        "# root mean squared error (rmse) for regression (only for Keras tensors)\n",
        "def rmse(y_true, y_pred):\n",
        "    from keras import backend\n",
        "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
        "\n",
        "# mean squared error (mse) for regression  (only for Keras tensors)\n",
        "def mse(y_true, y_pred):\n",
        "    from keras import backend\n",
        "    return backend.mean(backend.square(y_pred - y_true), axis=-1)\n",
        "\n",
        "# coefficient of determination (R^2) for regression  (only for Keras tensors)\n",
        "def r_square(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
        "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
        "\n",
        "# Compile model\n",
        "#model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.compile(optimizer=\"adam\", loss=rmse, metrics=[r_square, rmse, mse])\n",
        "\n",
        "# fit model\n",
        "history = model.fit(X, Y, epochs=100, batch_size=32, verbose=1)\n",
        "\n",
        "print('Done Training')\n",
        "\n",
        "# Visualize history\n",
        "# Plot history: Loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Validation loss history')\n",
        "plt.ylabel('Loss value')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUEWC5xRdAJq",
        "outputId": "67fdcd1c-6dfb-4c7a-d59b-ab4ff1b65a2f"
      },
      "source": [
        "model.layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.layers.core.Dense at 0x7f4b4c196890>,\n",
              " <keras.layers.core.Dense at 0x7f4b4c196690>,\n",
              " <keras.layers.core.Dense at 0x7f4b5f7f4510>,\n",
              " <keras.layers.core.Dense at 0x7f4b4c3496d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgnDZK8kJ0eD"
      },
      "source": [
        "**Evaluate dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsxVJEpY_RV7"
      },
      "source": [
        "# load dataset\n",
        "dataframe_test = read_csv(tomato_dataframe_test, sep=',', header=0)\n",
        "#dataframe = read_csv(tomato_dataframe, sep=';', header=0)\n",
        "dataset = dataframe_test.values\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "#X = dataframe[['distance','depth', 'pixels']].values\n",
        "X_test = dataframe[['depth', 'pixels']].values\n",
        "Y_test = dataframe[['weight']].values\n",
        "\n",
        "# Parse numbers as floats\n",
        "X_test = X_test.astype('float32')\n",
        "Y_test = Y_test.astype('float32')\n",
        "\n",
        "# Generate generalization metrics\n",
        "score = model.evaluate(X_test, Y_test)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qDkIx5JDIrV"
      },
      "source": [
        "history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WeyuKq_X5UL"
      },
      "source": [
        "# evaluate model with standardized dataset\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=32, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
        "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow9Wm67FYiU5"
      },
      "source": [
        "#Save the model\n",
        "model.save('/content/model/')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V2OfjRdNtEj"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgpzKqMoNtug"
      },
      "source": [
        "#Masked images folder\n",
        "%cd /content/model/\n",
        "\n",
        "#Zip all files in this folder\n",
        "!rm model.zip\n",
        "!zip model.zip *\n",
        "\n",
        "#Copy (.zip) files to (Google drive) destination\n",
        "!cp -r /content/model/model.zip /content/drive/MyDrive/mask_rcnn/masks/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMfwIxmmDOim"
      },
      "source": [
        "#Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezo8f2IdR3B6"
      },
      "source": [
        "# use the correct seperator depending on your dataset\n",
        "dataframe = read_csv(tomato_dataframe_test, sep=',', header=0)\n",
        "#dataframe = read_csv(tomato_dataframe, sep=';', header=0)\n",
        "dataset = dataframe.values\n",
        "\n",
        "#Xpredict = dataframe[['distance','depth', 'pixels']].values\n",
        "Xpredict = dataframe[['depth', 'pixels']].values\n",
        "Yreal = dataframe[['weight']].values\n",
        "totalerror =0\n",
        "                     \n",
        "# make a prediction\n",
        "ypredict = model.predict(Xpredict)\n",
        "# show the inputs and predicted outputs\n",
        "for i in range(len(Xpredict)):\n",
        "  error = abs(ypredict-Yreal[i])/Yreal[i]*100\n",
        "  print(\"X=%s, Predicted=%s, Real=%s, Error=%s%%\" % (Xpredict[i], ypredict[i], Yreal[i], error[i]))\n",
        "\n",
        "print('Average error =' + str(sum(error)/len(error))+'%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6x7yWeXSmja"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}